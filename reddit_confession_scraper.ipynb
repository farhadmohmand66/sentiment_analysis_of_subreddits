{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reddit Confession Scraper  \n",
    "\n",
    "## Overview  \n",
    "The **Reddit Scraper** is a web scraping tool designed to automate the process of logging into Reddit and extracting cofession from the community. Utilizing the Selenium WebDriver, this scraper navigates the Reddit website, interacts with the user interface, and collects relevant data from posts for analysis.  \n",
    "\n",
    "## How It Works  \n",
    "\n",
    "### Workflow Overview  \n",
    "\n",
    "1. **Driver Initialization**:  \n",
    "   - The scraper initializes a Selenium WebDriver instance configured to run in headless mode (without a graphical user interface).  \n",
    "   - It sets up necessary options for Chrome, including disabling GPU acceleration and configuring the browser window size.  \n",
    "\n",
    "2. **Login Process**:  \n",
    "   - The scraper navigates to the Reddit login page and enters the provided username and password.  \n",
    "   - It handles the login button, which is located within a Shadow DOM, using JavaScript to ensure successful interaction.  \n",
    "   - After clicking the login button, it checks the current URL to confirm a successful login.  \n",
    "\n",
    "3. **Scraping Posts**:  \n",
    "   - The scraper iterates through a list of specified confessin community.   \n",
    "\n",
    "4. **Data Extraction**:  \n",
    "   - The scraper scrolls through the search results, extracting relevant data from each post, including subreddit, title, content, author, and url. \n",
    "    \n",
    "5. **Data Storage**:  \n",
    "   - Extracted data is stored in a structured format csv for further analysis.  \n",
    "\n",
    "### Example of Extracted Data  \n",
    "\n",
    "The scraper collects the following types of data for each post:  \n",
    "\n",
    "| Field          | Description                                    |  \n",
    "|----------------|------------------------------------------------|  \n",
    "| subreddit      | Name of the specified confessson communigy     |  \n",
    "| title          | The title of the post                          |  \n",
    "| content        | The main body text of the post                 |  \n",
    "| author         | The author of the post                         |  \n",
    "| url            | link of of post                                |  \n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "## Important Notes  \n",
    "\n",
    "- **Temporary Accounts**: It is recommended to use temporary accounts for scraping to avoid potential violations of Reddit's tags of service.  \n",
    "- **Dynamic Content**: The scraper relies on specific XPath and CSS selectors to extract data. Changes in Reddit's layout may require updates to these selectors.   \n",
    "\n",
    "## Conclusion  \n",
    "\n",
    "The Reddit Scraper provides an efficient way to gather and analyze social media data from the Reddit platform. By automating the login and data extraction processes, it enables users to gain valuable insights into trends and discussions in a timely manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize WebDriver with or wihtout Headless Mode\n",
    "def init_driver(chromedriver_path):\n",
    "    chrome_options = Options()\n",
    "    # chrome_options.add_argument(\"--headless\")\n",
    "    chrome_options.add_argument(\"--disable-gpu\")\n",
    "    chrome_options.add_argument(\"--no-sandbox\")\n",
    "    chrome_options.add_argument(\"--log-level=3\")\n",
    "    chrome_options.add_argument(\"--disable-web-security\")  # Disable web security \n",
    "    chrome_options.add_argument(\"--window-size=1280,1024\")\n",
    "    chrome_options.add_argument(\"--ignore-certificate-errors\")  # Ignore SSL errors  \n",
    "    chrome_options.add_argument(\"--allow-insecure-localhost\")  # Allow insecure localhost connections\n",
    "    driver = webdriver.Chrome(executable_path=chromedriver_path, options=chrome_options)\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    print(\"driver intiated successfuly\")\n",
    "    return driver, wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver_path = r\"D:\\coding\\freelancing\\stockMarket\\chromedriver.exe\"\n",
    "driver, wait = init_driver(chromedriver_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log In to Reddit\n",
    "def login_to_reddit(driver, wait, username, password):\n",
    "    \"\"\"\n",
    "    Log in to Reddit with provided credentials.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(\"https://www.reddit.com/login/\")\n",
    "        time.sleep(5)\n",
    "        # Enter username\n",
    "        username_input = wait.until(EC.presence_of_element_located((By.ID, \"login-username\")))\n",
    "        username_input.send_keys(username)\n",
    "\n",
    "        # Enter password\n",
    "        password_input = wait.until(EC.presence_of_element_located((By.ID, \"login-password\")))\n",
    "        password_input.send_keys(password)\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "        # Locate the login button inside the Shadow DOM using JavaScript\n",
    "            login_button = driver.execute_script(\"\"\"\n",
    "                return document\n",
    "                    .querySelector(\"body > shreddit-app > shreddit-overlay-display\")\n",
    "                    .shadowRoot.querySelector(\"shreddit-signup-drawer\")\n",
    "                    .shadowRoot.querySelector(\"shreddit-drawer > div > shreddit-async-loader > div > shreddit-slotter\")\n",
    "                    .shadowRoot.querySelector(\"#login > auth-flow-modal > div.w-100 > faceplate-tracker > button\");\n",
    "            \"\"\")\n",
    "\n",
    "            if login_button:\n",
    "                # Click the login button\n",
    "                driver.execute_script(\"arguments[0].click();\", login_button)\n",
    "                print(\"Login button clicked successfully!\")\n",
    "        except Exception:\n",
    "            # Fallback to JavaScript click if standard click fails\n",
    "            login_button_js = driver.find_element(By.XPATH, '//*[@id=\"login\"]/auth-flow-modal/div[2]/faceplate-tracker/button/span/span')\n",
    "            driver.execute_script(\"arguments[0].click();\", login_button_js)\n",
    "            print(\"Login button clicked using JavaScript!\")\n",
    "\n",
    "            # Wait for successful login\n",
    "            time.sleep(5)\n",
    "            if \"login\" in driver.current_url.lower():\n",
    "                raise Exception(\"Login failed. Still on login page.\")\n",
    "                print(\"Login successful!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during Reddit login: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credentials and configuration\n",
    "username = \"uname\"\n",
    "password = \"pass\"\n",
    "login_to_reddit(driver, wait, username, password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def click_all_collapsed_expando_buttons(driver):\n",
    "    \"\"\"\n",
    "    Finds all buttons with class 'expando-button' and clicks those that are collapsed.\n",
    "    \n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Find all elements with class name \"expando-button\"\n",
    "        expando_buttons = driver.find_elements(By.CLASS_NAME, \"expando-button\")\n",
    "        for btn in expando_buttons:\n",
    "            # Check if the button has class \"collapsed\"\n",
    "            if \"collapsed\" in btn.get_attribute(\"class\"):\n",
    "                # Click using JavaScript\n",
    "                driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                time.sleep(0.5)  # small delay to mimic natural clicking\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while clicking: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_post_content(post):\n",
    "    \"\"\"Extract text content with multiple fallback methods\"\"\"\n",
    "    content = \"\"\n",
    "    \n",
    "    # Method 1: Direct text extraction\n",
    "    try:\n",
    "        content_div = post.find_element(By.CSS_SELECTOR, \"div.md\")\n",
    "        content = content_div.text\n",
    "    except Exception:\n",
    "        pass\n",
    "    \n",
    "    # Method 2: Handle cross-posts\n",
    "    if not content:\n",
    "        try:\n",
    "            crosspost_content = post.find_element(By.CSS_SELECTOR, \"div.crosspost-preview\")\n",
    "            content = crosspost_content.text\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Method 3: Fallback to usertext body\n",
    "    if not content:\n",
    "        try:\n",
    "            usertext_div = post.find_element(By.CSS_SELECTOR, \"div.usertext-body\")\n",
    "            content = usertext_div.text\n",
    "        except Exception:\n",
    "            pass\n",
    "    \n",
    "    # Clean and return content\n",
    "    return clean_text(content) if content else \"\"\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Prepare text for translation\"\"\"\n",
    "    # Remove Reddit-specific formatting\n",
    "    text = re.sub(r\"\\[deleted\\]|\\[removed\\]\", \"\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\*{2}(.*?)\\*{2}\", r\"\\1\", text)  # Remove bold formatting\n",
    "    text = re.sub(r\"\\*(.*?)\\*\", r\"\\1\", text)  # Remove italic formatting\n",
    "    text = re.sub(r\"~~(.*?)~~\", r\"\\1\", text)  # Remove strikethrough\n",
    "    text = re.sub(r\"^>.*$\", \"\", text, flags=re.MULTILINE)  # Remove blockquotes\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()  # Normalize whitespace\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_subreddit(subreddit, pages=10, min_content_length=20):\n",
    "    \"\"\"Scrape a subreddit with improved content extraction\"\"\"\n",
    "    print(f\"Scraping r/{subreddit}...\")\n",
    "    data = []\n",
    "    url = f\"https://old.reddit.com/r/{subreddit}/top/?sort=top&t=all\"\n",
    "    driver.get(url)\n",
    "    \n",
    "    for _ in range(pages):\n",
    "        try:\n",
    "            # Wait for posts to load\n",
    "            wait.until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"thing\"))\n",
    "            )\n",
    "            click_all_collapsed_expando_buttons(driver)\n",
    "        except Exception:\n",
    "            print(\"Timeout waiting for posts\")\n",
    "            break\n",
    "        time.sleep(4)\n",
    "        click_all_collapsed_expando_buttons(driver)\n",
    "        time.sleep(2)\n",
    "        posts = driver.find_elements(By.CLASS_NAME, \"thing\")\n",
    "        \n",
    "        for post in posts:\n",
    "            try:\n",
    "                # Skip promoted ads\n",
    "                if \"promoted\" in post.get_attribute(\"class\"):\n",
    "                    continue\n",
    "                title = post.find_element(By.CLASS_NAME, \"title\").text\n",
    "                try:\n",
    "                    author = post.get_attribute(\"data-author\")\n",
    "                except:\n",
    "                    author = \"N/a\"\n",
    "                post_url = post.get_attribute(\"data-permalink\")\n",
    "                \n",
    "                content = extract_post_content(post)\n",
    "                \n",
    "                # Skip if content is too short\n",
    "                if len(content) < min_content_length:\n",
    "                    continue\n",
    "                \n",
    "                # Add to dataset\n",
    "                data.append({\n",
    "                    \"subreddit\": subreddit,\n",
    "                    \"title\": title,\n",
    "                    \"content\": content,\n",
    "                    \"author\": author,\n",
    "                    \"url\": \"https://old.reddit.com\" + post_url\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing post: {str(e)[:100]}\")\n",
    "                continue\n",
    "        \n",
    "        # Go to next page\n",
    "        try:\n",
    "            next_button = driver.find_element(By.LINK_TEXT, \"next ›\")\n",
    "            driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "            time.sleep(2)  # Important: let page load\n",
    "        except Exception:\n",
    "            print(\"No more pages\")\n",
    "            break\n",
    "            \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping r/confession...\n",
      "Timeout waiting for posts\n",
      "Scraped 94 posts from r/confession\n",
      "Total posts scraped: 94\n",
      "Saved to reddit_confessions_20250622_155344.csv\n"
     ]
    }
   ],
   "source": [
    "# Main scraping process\n",
    "if __name__ == \"__main__\":\n",
    "    subreddits = [\"confession\"]\n",
    "    # subreddits = [\"confessions\", \"offmychest\", \"TrueOffMyChest\", \"guilt\", \"regret\"]\n",
    "    all_data = []\n",
    "    \n",
    "    for sub in subreddits:\n",
    "        try:\n",
    "            sub_data = scrape_subreddit(sub)\n",
    "            all_data.extend(sub_data)\n",
    "            print(f\"Scraped {len(sub_data)} posts from r/{sub}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to scrape r/{sub}: {str(e)}\")\n",
    "    \n",
    "    # Create DataFrame and save\n",
    "    if all_data:\n",
    "        df = pd.DataFrame(all_data)\n",
    "        print(f\"Total posts scraped: {len(df)}\")\n",
    "        \n",
    "        # Final cleaning\n",
    "        df[\"content\"] = df[\"content\"].apply(clean_text)\n",
    "        df = df[df[\"content\"].str.len() > 50]  # Final length filter\n",
    "        \n",
    "        # Save results\n",
    "        timestamp = pd.Timestamp.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"reddit_confessions_{timestamp}.csv\"\n",
    "        df.to_csv(filename, index=False, encoding='utf-8-sig')\n",
    "        print(f\"Saved to {filename}\")\n",
    "    else:\n",
    "        print(\"No data scraped\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
